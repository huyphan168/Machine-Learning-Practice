# -*- coding: utf-8 -*-
"""Decision Tree from scratch.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ErGGhJao3_OzIK34kBKMgDm6lSNy8pzU
"""

import numpy as np
import csv

class InternalNode(object):
    # An Internal Node class that has an associated feature and criteria for splitting. 
    def __init__(self, feature = None, criteria = None): # Constructor
        self.is_leaf = False
        self.feature = feature
        self.criteria = criteria
        self.left = None
        self.right = None
        self.data = None

    def insert_left(self, child):
        if self.left is None:
            self.left = child
        else:
            child.left = self.left
            self.left = child

    def insert_right(self, child):
        if self.right is None:
            self.right = child
        else:
            child.right = self.right
            self.right = child
    
    def get_depth(self, iter):
        # Recursively return the depth of the node.
        l_depth = self.left.get_depth(iter+1)
        r_depth = self.right.get_depth(iter+1)
        return max([l_depth, r_depth])
    #A node has split method to split the data based on feature and criteria 
    def split(self, data, feature, criteria):
        set1 = []
        set2 = []
        for row in data:
          if row[feature] < criteria:
            set1.append(row)
          else:
            set2.append(row)
        return set1, set2
    
    
class LeafNode(object):
    # A Leaf Node class that has an associated decision.
    def __init__(self, decision): # Constructor
        self.decision = decision
        self.is_leaf = True

    def retreiveDecision(self):
        return self.decision

    def get_depth(self, iter):
        return iter


class DecisionTreeBuilder:
  '''This is a Python class named DecisionTreeBuilder.'''
  def __init__(self): # Constructor
    self.tree = None # Define a ``tree'' instance variable.
    #thresh_hold for depth of the tree
    self.threshold = 20
    #thresh_hold for dominant label in splitted data( label_dominant/(len(data)) )
    self.BING = 0.92

  #Construct method: Building a decision tree based on given data
  def construct(self, data):
    #Initialize the root of our tree
    root = InternalNode()
    #Assign original data for root node
    root.data = data
    #Recursively sparsing data to bulid a node
    self.sparsing_node(root, 1)
    #Assign the root to the tree because the root has completed branch.
    self.tree = root
    return self.tree.get_depth(0) 

  #This method is to check whether a node is a leaf_node based on its data after splitted from parent node
  def check_leaf_node(self,data):
    if len(data) == 0:
      return True, 4
    label = []
    for row in data:
      label.append(row[-1])
    #Counting number of class 1 and class 2
    label1 = len([class_ for class_ in label if class_ == 2])
    label2 = len([class_ for class_ in label if class_ == 4])
    """"
    If in a node that has dominant label (more than 92% of data),
    we suppose it was a leaf_node because it is not necessary to
    split the data more. Finally return the dominant label as a 
    final decision for that leaf_node.
    """"
    if label1/(label1+label2) > self.BING or label2/(label1+label2) > self.BING:
      if label1 > label2:
        return True, 2
      else:
        return True, 4
    """"
    If there isnt any dominant label, we see it as an internal node
    for continuing split. However, if the tree reaches the max_depth,
    we still need this function to return the decision of this node 
    (the more frequent label). This is effective for later assign to 
    the leaf_node if the tree reaches max_depth.
    """"
    else:
      if label1 > label2:
        return False, 2
      else:
        return False, 4
  """'
  This is our main method and it is called recursively in our constuct method.
  The sparsing_node has input including a node and max_depth. It sparse the feature
  and criteria then split data in a node, computes the information gain from that split.
  MAX_IG will contain all related information of a split that results maximum information
  gain.
  """"
  def sparsing_node(self,node, max_depth):
    MAX_IG = {"left_data": None, "right_data": None, "feature": None, "criteria": None, "IG": -1000}
    for feature in range(1,10):
      for discrete_value in range(1,11):
        #Trying a combination of feature and criteria to split the data
        set1, set2 = node.split(node.data, feature, discrete_value)
        #Compute information gain of a split
        IG = self.information_gain(node.data, set1, set2)
        #Assign a split that has greater information gain to the MAX_IG 
        if IG > MAX_IG["IG"]:
          MAX_IG["left_data"] = set1
          MAX_IG["right_data"] = set2
          MAX_IG["feature"] = feature
          MAX_IG["criteria"] = discrete_value
          MAX_IG["IG"] = IG
    #After finding the best split, we assign the found feature and criteria to the node for later predictions.
    node.feature, node.criteria = MAX_IG["feature"], MAX_IG["criteria"]
    #Delete the node'data to release the memory
    node.data = None
    #Check a splitted datasets were a leaf_node
    left_is_leaf, left_value = self.check_leaf_node(MAX_IG["left_data"])
    right_is_leaf, right_value = self.check_leaf_node(MAX_IG["right_data"])
    #If the tree reaches the max_depth, we insert 2 leaf_nodes and ending the recursion
    if max_depth >= self.threshold:
      node.insert_left(LeafNode(left_value))
      node.insert_right(LeafNode(right_value))
      return
    #if the tree does not reach the max_depth and the left_node has a dominant class then we assign it as a leaf_node
    #and insert that leaf_node to the parent node
    if left_is_leaf:
      left_leaf = LeafNode(left_value)
      node.insert_left(left_leaf)
    #If the tree does not reach the max_depth and the left_node does not have any dominant class (still blended)
    #We assign it as a internal node for continuing split
    else:
      left_node = InternalNode()
      left_node.data = MAX_IG["left_data"]
      node.insert_left(left_node)
      #Continue finding the split for this internal node
      self.sparsing_node(left_node, max_depth + 1)
    #Same as left_node
    if right_is_leaf:
      right_leaf = LeafNode(right_value)
      node.insert_right(right_leaf)
    else:
      right_node = InternalNode()
      right_node.data = MAX_IG["right_data"]
      node.insert_right(right_node)
      self.sparsing_node(right_node, max_depth + 1)
  #Computing information gain from a split
  def information_gain(self, data, set1, set2):
    IG = self.entropy(data) 
    for x in [set1, set2]:
      if len(x) != 0:
        IG -= len(x)*self.entropy(x)
    return IG
  #Computing entropy of the data
  def entropy(self, data):
    if len(data) == 0:
      return 0
    label = []
    for row in data:
      label.append(row[-1])
    num_class_1 = len([class_ for class_ in label if class_ == 2])
    num_class_2 = len([class_ for class_ in label if class_ == 4])
    num = num_class_1 + num_class_2
    return -(num_class_1/num)*np.log(num_class_1/num) - (num_class_2/num)*np.log(num_class_2/num)

  #Prediction method (recursively called)
  def predict(self, node, row):
    if row[node.feature] < node.criteria:
      if node.left.is_leaf is not True:
        return self.predict(node.left, row)
      else:
        return node.left.retreiveDecision()
    else:
      if node.right.is_leaf is not True:
        return self.predict(node.right, row)
      else:
        return node.right.retreiveDecision()
        
  def classify(self, data):
    predictions = []
    node = self.tree
    for row in data:
      predictions.append(self.predict(node, row))
    return predictions
    
print("1. Reading File")
with open("/content/breast-cancer-wisconsin.data") as fp:
    reader = csv.reader(fp, delimiter=",", quotechar='"')
    # Uncomment the following line if the first line in your CSV is column names
    # next(reader, None)  # skip the headers
    
    # create a list (i.e. array) where each index is a row of the CSV file.
    all_data = [row for row in reader]
for i in range(len(all_data)):
  for j in range(11):
    if all_data[i][j] == "?":
      all_data[i][j] = 0
    all_data[i][j] = int(all_data[i][j])

# 2. Split the data into training and test sets.
#   Note: This is an example split that simply takes the first 90% of the
#    data read in as training data and uses the remaining 10% as test data. 
print("2. Separating Data")
number_of_rows = len(all_data)  
training_data = all_data[:600]
test_data = all_data[:600]         
print()

# 3. Create an instance of the DecisionTreeBuilder class.
print("3. Instantiating DecisionTreeBuilder")
dtb = DecisionTreeBuilder()
print()

# 4. Construct the Tree.
print("4. Constructing the Tree with Training Data")
tree_length = dtb.construct(training_data)
print("Tree Length: " + str(tree_length))
print()

# 5. Classify Test Data using the Tree.
print("5. Classifying Test Data with the Constructed Tree")
predictions = dtb.classify(test_data)
print()

accurate = 0
if(len(predictions) > 0):
    for idx, prediction in enumerate(predictions):
        if prediction == test_data[idx][-1]:
          accurate += 1
        
else:
    print(' : Predictions list is empty.')
print("THE ACCURACY OF THE DECISION TREE IS: {} %".format(float(accurate/len(predictions)*100)))

